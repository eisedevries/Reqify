# main.py

import os
import json
import csv
import logging
import re
from pathlib import Path
from dotenv import load_dotenv
from openai import AzureOpenAI
from tqdm import tqdm

# PROMPTS
SYSTEM_ANALYST_PROMPT = """
**System Role:** You are an expert Systems Analyst. Your task is to carefully analyze an interview transcript and extract all system requirements mentioned by the speaker.

**Task:**
From the provided interview transcript, elicit all requirements (include both functional and non-functional requirements, you do not need to specify or categorize). Your primary goal is to capture the user's needs as accurately and completely as possible.

**Context Provided:**
1.  `{interview_transcript}`: The original source transcript.

**Constraint: Do not elicit more than 30 requirements.** Focus only on the most critical and distinct user needs mentioned in the transcript.

**Guidelines for High-Quality Requirements:**
To ensure the requirements you extract are clear and useful, please follow these best practices:
- **Clarity:** The requirement should have only one clear interpretation.
- **Atomicity:** Each requirement should describe a single, distinct need. Avoid combining multiple ideas into one statement.
- **Completeness:** The requirement statement should be a full sentence that expresses a complete thought.
- **Source-Based:** Every requirement you list must be directly supported by a statement or a strong implication from the transcript.

**Output Format:**
You MUST produce your output as a single, valid JSON object. The root key must be "requirements", which contains a list of individual requirement objects. Provide only the JSON and no other commentary.

Each requirement object in the list must have the following structure:
- `id`: A simple, unique identifier string (e.g., "R1", "R2").
- `statement`: The requirement text, written as a clear, formal sentence (e.g., "The system shall...").
- `source_quote`: A direct, verbatim quote from the transcript that justifies the requirement, ensuring traceability. Give a literal quote, not a summary or paraphrase. Copy the exact words used by the speaker.

**Example of the required JSON structure:**
```json
{{
  "requirements": [
    {{
      "id": "R1",
      "statement": "The system shall allow a user to register for a new account using an email address and password.",
      "source_quote": "So, obviously, a new user needs to be able to sign up, you know, just with their email."
    }},
    {{
      "id": "R2",
      "statement": "The system must load the main dashboard in under 2 seconds.",
      "source_quote": "It's just got to be fast, I can't be waiting around for the main page to load."
    }}
  ]
}}
"""

METACOGNITIVE_PROMPT_ENGINEER_PROMPT = """
**System Role:** You are a Metacognitive Prompt Engineer. Your function is to analyze and improve the performance of another AI agent tasked with eliciting system requirements from transcripts.

**Task:**
You will be given the list of requirements an AI agent produced, the original transcript, and the prompt that was used. Your task is to identify weaknesses in the original prompt by analyzing the agent's output, and then rewrite the prompt to elicit a more accurate and complete set of requirements on the next attempt.

**Context Provided:**
1.  `{requirements_list}`: The JSON list of requirements generated by the previous agent.
2.  `{interview_transcript}`: The original source transcript the agent worked from.
3.  `{original_prompt}`: The exact prompt that was given to the previous agent.

---

### Your Process

**Step 1: Perform a Gap Analysis**
Critically compare the agent's output (`{requirements_list}`) against the source material (`{interview_transcript}`). Your goal is to identify discrepancies and missed information. Focus on the following:
-   **Missing Requirements:** Identify any user needs clearly stated or strongly implied in the transcript that were not captured in the requirements list.
-   **Misinterpreted Requirements:** Find any requirements in the list that misunderstand or inaccurately represent what the user said in the transcript.
-   **Compound Requirements:** Note any instances where a single requirement statement improperly combines two or more distinct user needs.
-   **Poor Phrasing:** Identify requirements that are vague, unclear, or poorly written, even if based on the transcript.
-   **Traceability Errors:** Check if the `source_quote` for each requirement is accurate and truly supports the requirement statement. The quotes must be verbatim from the transcript.

**Step 2: Formulate an Improvement Strategy & Focus on Self-Correction**
- Based on the gaps you identified, decide how the `{original_prompt}` should be changed.
- In your analysis, you MUST explicitly state the primary failure of the previous attempt (e.g., "The main failure was missing non-functional requirements about performance.").
- Your new prompt's instructions MUST be specifically designed to correct that primary failure.

**Step 3: Construct the Improved Prompt**
Rewrite the `{original_prompt}` from scratch. Your new prompt must be a complete, self-contained set of instructions designed to fix the errors you identified. Make sure the quotes are verbatim from the transcript.

**Crucial Constraints for the New Prompt:**
-   **Preserve JSON Output:** The new prompt **must** instruct the System Analyst agent to provide its output in the exact same JSON format as the original. This is a non-negotiable part of the output format.
-   **Make it a Complete Template:** The new prompt must be a full and complete prompt, ready for immediate use, including the `{interview_transcript}` placeholder.

---

**Your Output:**
Please provide your response in two parts: first, your analysis, and second, the improved prompt.

**Analysis of Previous Attempt:**
(Provide your findings from Step 1 and your self-correction strategy from Step 2 here.)

**Improved Prompt:**
(Provide the complete text of your new, rewritten prompt here.)

**Begin your analysis and prompt rewrite now.**
"""

# --- CONSTANTS ---
TRANSCRIPTS_DIR = Path("transcripts")
RESULTS_DIR = Path("results")
LOG_FILE = "progress.log"
SINGLE_RESULTS_FILE = RESULTS_DIR / "single_results.csv"
META_RESULTS_FILE = RESULTS_DIR / "meta_results.csv"
META_ITERATIONS = 3
MAX_REQUIREMENTS_FOR_HEADER = 30
MAX_RETRIES = 3

# --- SETUP FUNCTIONS ---

def setup_logging():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(LOG_FILE, mode='a'),
            logging.StreamHandler()
        ]
    )

def load_env_and_create_client():
    load_dotenv()
    try:
        client = AzureOpenAI(
            api_version=os.getenv("api_version"),
            azure_endpoint=os.getenv("azure_endpoint"),
            api_key=os.getenv("api_key"),
        )
        client.models.list()
        logging.info("Azure OpenAI client created and credentials verified.")
        return client
    except Exception as e:
        logging.error(f"Failed to create Azure OpenAI client. Check .env file and credentials. Error: {e}")
        return None

# --- LLM & PARSING FUNCTIONS ---

def call_llm(client, messages, model_name, deployment_name, is_json_output=True, timeout=60.0):
    """Makes a call to the LLM and returns the response content."""
    try:
        response_format = {"type": "json_object"} if is_json_output else None
        
        response = client.chat.completions.create(
            model=deployment_name,
            messages=messages,
            response_format=response_format,
            timeout=timeout
        )
        return response.choices[0].message.content
    except Exception as e:
        logging.error(f"LLM API call failed: {e}")
        return None

def clean_and_parse_json(response_text: str):
    if not response_text:
        logging.error("LLM returned an empty response.")
        return None
    
    match = re.search(r"```json\s*([\s\S]*?)\s*```", response_text)
    if match:
        json_str = match.group(1)
    else:
        json_str = response_text
    
    try:
        data = json.loads(json_str)
        if "requirements" in data and isinstance(data["requirements"], list):
            return data["requirements"]
        else:
            logging.error("Parsed JSON is missing the 'requirements' list.")
            return None
    except json.JSONDecodeError as e:
        logging.error(f"Failed to decode JSON from LLM response. Error: {e}")
        logging.debug(f"Problematic response text: {response_text}")
        return None

def parse_meta_response(response_text: str):
    if not response_text:
        logging.error("Meta-cognitive LLM returned an empty response.")
        return None, None
        
    try:
        response_text = response_text.replace("**Improved Prompt:**", "Improved Prompt:")
        response_text = response_text.replace("**Analysis of Previous Attempt:**", "Analysis of Previous Attempt:")

        if "Improved Prompt:" in response_text:
            parts = response_text.split("Improved Prompt:", 1)
            analysis = parts[0].replace("Analysis of Previous Attempt:", "").strip()
            improved_prompt = parts[1].strip()
            return analysis, improved_prompt
        else:
            logging.error("Could not find 'Improved Prompt:' separator in meta-response.")
            return response_text, None
    except Exception as e:
        logging.error(f"Error parsing meta-cognitive response: {e}")
        return None, None

# --- DATA PREPARATION & UTILS ---

def format_transcript(transcript_content: dict) -> str:
    formatted_lines = []
    if "messages" in transcript_content and isinstance(transcript_content["messages"], list):
        for message in transcript_content["messages"]:
            if message.get("position") == 0 and message.get("message_type") == 0:
                continue
            
            speaker = "Stakeholder" if message.get("message_type") == 2 else "Interviewer"
            text = message.get("message_text", "").strip()
            if text:
                formatted_lines.append(f"{speaker}: {text}")
    
    return "\n".join(formatted_lines)

def make_prompt_safe_for_format(prompt_text: str, placeholders: list) -> str:
    temp_replacements = {}
    for i, p in enumerate(placeholders):
        token = f"__PLACEHOLDER_{i}__"
        temp_replacements[token] = f"{{{p}}}"
        prompt_text = prompt_text.replace(f"{{{p}}}", token)

    prompt_text = prompt_text.replace("{", "{{").replace("}", "}}")

    for token, original_placeholder in temp_replacements.items():
        prompt_text = prompt_text.replace(token, original_placeholder)

    return prompt_text

# --- CSV & STATE MANAGEMENT ---

def get_processed_ids(filepath, is_meta=False):
    if not filepath.exists():
        return set()
    
    processed = set()
    with open(filepath, 'r', encoding='utf-8') as f:
        reader = csv.reader(f, delimiter=';')
        try:
            next(reader) 
            for row in reader:
                if not row: continue
                if is_meta:
                    if len(row) > 1:
                        processed.add((row[0], row[1]))
                else:
                    processed.add(row[0])
        except StopIteration:
            return set()
    return processed

def prepare_csv(filepath, headers):
    RESULTS_DIR.mkdir(exist_ok=True)
    if not filepath.exists() or os.path.getsize(filepath) == 0:
        with open(filepath, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f, delimiter=';', quoting=csv.QUOTE_ALL)
            writer.writerow(headers)
        logging.info(f"Created new results file: {filepath}")

# --- WORKFLOWS ---

def run_single_prompt_workflow(client):
    logging.info("--- Starting Single-Prompt Workflow ---")
    model_name = os.getenv("model_name")
    deployment_name = os.getenv("deployment")

    headers = ['Interview ID']
    for i in range(1, MAX_REQUIREMENTS_FOR_HEADER + 1):
        headers.extend([f'R{i}_REQ', f'R{i}_QT'])
    prepare_csv(SINGLE_RESULTS_FILE, headers)

    processed_ids = get_processed_ids(SINGLE_RESULTS_FILE)
    all_files = list(TRANSCRIPTS_DIR.glob("*.json"))
    files_to_process = [f for f in all_files if f.stem not in processed_ids]
    
    if not files_to_process:
        logging.info("All transcripts already processed for single-prompt workflow.")
        return

    logging.info(f"Found {len(files_to_process)} new transcripts to process.")
    
    for transcript_path in tqdm(files_to_process, desc="Single-Prompt Processing"):
        interview_id = transcript_path.stem
        logging.info(f"Processing {interview_id}...")
        
        with open(transcript_path, 'r', encoding='utf-8') as f:
            transcript_content = json.load(f)
            transcript_text = format_transcript(transcript_content)

        if not transcript_text:
            logging.warning(f"Transcript {interview_id} is empty or invalid. Skipping.")
            continue

        final_prompt = SYSTEM_ANALYST_PROMPT.format(interview_transcript=transcript_text)
        messages = [{"role": "user", "content": final_prompt}]
        
        response_content = call_llm(client, messages, model_name, deployment_name, is_json_output=True)
        requirements = clean_and_parse_json(response_content)

        if requirements is not None:
            row = [interview_id]
            for req in requirements:
                row.extend([req.get('statement', ''), req.get('source_quote', '')])
            
            with open(SINGLE_RESULTS_FILE, 'a', newline='', encoding='utf-8') as f:
                writer = csv.writer(f, delimiter=';', quoting=csv.QUOTE_ALL)
                writer.writerow(row)
            logging.info(f"Successfully processed and saved requirements for {interview_id}.")
        else:
            logging.warning(f"Skipping CSV entry for {interview_id} due to processing error.")

def run_meta_prompt_workflow(client):
    logging.info("--- Starting Meta-Prompt Workflow ---")
    model_name = os.getenv("model_name")
    deployment_name = os.getenv("deployment")
    
    headers = ['Interview ID', 'Iteration', 'Given Prompt', 'Analysis']
    for i in range(1, MAX_REQUIREMENTS_FOR_HEADER + 1):
        headers.extend([f'R{i}_REQ', f'R{i}_QT'])
    prepare_csv(META_RESULTS_FILE, headers)

    processed_tuples = get_processed_ids(META_RESULTS_FILE, is_meta=True)
    all_files = list(TRANSCRIPTS_DIR.glob("*.json"))

    logging.info(f"Found {len(all_files)} total transcripts for meta-prompt workflow.")

    for transcript_path in tqdm(all_files, desc="Meta-Prompt Processing (Transcripts)"):
        interview_id = transcript_path.stem
        logging.info(f"Starting meta-prompt process for {interview_id}...")
        
        with open(transcript_path, 'r', encoding='utf-8') as f:
            transcript_content = json.load(f)
            transcript_text = format_transcript(transcript_content)
        
        if not transcript_text:
            logging.warning(f"Transcript {interview_id} is empty or invalid. Skipping.")
            continue

        current_prompt = SYSTEM_ANALYST_PROMPT
        analysis = "N/A"

        for iteration in range(1, META_ITERATIONS + 1):
            if (interview_id, str(iteration)) in processed_tuples:
                logging.info(f"Skipping {interview_id} - Iteration {iteration} (already processed).")
                continue

            iteration_successful = False
            # --- NEW: Retry loop for each iteration ---
            for attempt in range(1, MAX_RETRIES + 1):
                logging.info(f"Processing {interview_id} - Iteration {iteration} (Attempt {attempt}/{MAX_RETRIES})")

                # --- 1. Analyst Elicitation Step ---
                final_analyst_prompt = current_prompt.format(interview_transcript=transcript_text)
                analyst_messages = [{"role": "user", "content": final_analyst_prompt}]
                response_content = call_llm(client, analyst_messages, model_name, deployment_name, is_json_output=True)
                requirements = clean_and_parse_json(response_content)

                if requirements is None:
                    logging.warning(f"Analyst call failed on attempt {attempt} for Iteration {iteration}.")
                    continue # Go to the next attempt

                # --- 2. Meta-Prompt Rewrite Step (if needed) ---
                next_prompt = current_prompt
                next_analysis = analysis

                if iteration < META_ITERATIONS:
                    previous_findings_json = json.dumps(requirements, indent=2)
                    meta_prompt = METACOGNITIVE_PROMPT_ENGINEER_PROMPT.format(
                        requirements_list=previous_findings_json,
                        interview_transcript=transcript_text,
                        original_prompt=current_prompt
                    )
                    meta_messages = [{"role": "user", "content": meta_prompt}]
                    
                    meta_response_content = call_llm(client, meta_messages, model_name, deployment_name, is_json_output=False, timeout=300.0)
                    
                    if meta_response_content is None:
                        logging.warning(f"Meta-call failed on attempt {attempt} for Iteration {iteration}.")
                        continue # Go to the next attempt

                    parsed_analysis, new_prompt = parse_meta_response(meta_response_content)

                    if new_prompt is None:
                        logging.warning(f"Meta-response parsing failed on attempt {attempt} for Iteration {iteration}.")
                        continue # Go to the next attempt
                    
                    next_analysis = parsed_analysis
                    next_prompt = make_prompt_safe_for_format(new_prompt, ['interview_transcript'])
                
                # --- 3. Success: Commit results and break retry loop ---
                logging.info(f"Iteration {iteration} succeeded on attempt {attempt}.")
                
                sanitized_prompt = current_prompt.replace(';', ',')
                sanitized_analysis = analysis.replace(';', ',')
                row = [interview_id, iteration, sanitized_prompt, sanitized_analysis]
                for req in requirements:
                    row.extend([req.get('statement', ''), req.get('source_quote', '')])
                
                with open(META_RESULTS_FILE, 'a', newline='', encoding='utf-8') as f:
                    writer = csv.writer(f, delimiter=';', quoting=csv.QUOTE_ALL)
                    writer.writerow(row)
                logging.info(f"Saved results for {interview_id} - Iteration {iteration}")

                # Update state for the next outer loop iteration
                current_prompt = next_prompt
                analysis = next_analysis
                iteration_successful = True
                break # Exit the retry loop
            
            # If all retry attempts for an iteration failed, abort for this transcript.
            if not iteration_successful:
                logging.error(f"Iteration {iteration} failed after {MAX_RETRIES} attempts. Aborting for this transcript.")
                break # Exit the main iteration loop

# --- MAIN EXECUTION ---

if __name__ == "__main__":
    setup_logging()
    
    if not SYSTEM_ANALYST_PROMPT.strip() or not METACOGNITIVE_PROMPT_ENGINEER_PROMPT.strip():
        logging.error("Prompts are not filled in. Please paste the prompts into the script.")
    else:
        choice = ""
        while choice not in ['1', '2', '3']:
            choice = input("Select an option:\n[1] Run single prompt\n[2] Run meta-prompt\n[3] Run both\nEnter choice (1, 2, or 3): ")

        client = load_env_and_create_client()
        if client:
            if choice == '1':
                run_single_prompt_workflow(client)
            elif choice == '2':
                run_meta_prompt_workflow(client)
            elif choice == '3':
                run_single_prompt_workflow(client)
                run_meta_prompt_workflow(client)
            logging.info("Script finished.")
        else:
            logging.error("Exiting due to client initialization failure.")